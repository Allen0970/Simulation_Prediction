{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3622dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom.minidom import parse\n",
    "\n",
    "import unittest\n",
    "import os\n",
    "import time\n",
    "import xml.dom.minidom\n",
    "import pandas as pd\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "330fcdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path is: /Users/xuzizhuo/Desktop/Main Folder/My_works/routeSys_Github/Model_Training_on_Manual_Allocation_Data/SUMO_Simulation/test.net.xml\n",
      "Number of edges are:  29497\n"
     ]
    }
   ],
   "source": [
    "# 1. Capture network information from \".xml\" and save into \".csv\"\n",
    "# SUMO's  network contains connection information. This code extracts the following required information from it:\n",
    "# 1). edge ID,\n",
    "# 2). Features corresponding to edge ID (road length, speed limit, lanes number),\n",
    "# 3). Two node IDs connected to edge ID,\n",
    "\n",
    "def network_data_exploration(filename):\n",
    "    # Define path\n",
    "    path = os.path.abspath('./../../../Realistic_Event_Data_Generation_Procedure/') \n",
    "    # Define data path\n",
    "    data_path = os.path.join(path,filename) \n",
    "    print('Data path is:', data_path) \n",
    "    \n",
    "    # Open \".xml\" and find data\n",
    "    DOMTree = xml.dom.minidom.parse(data_path) \n",
    "    data = DOMTree.documentElement \n",
    "    \n",
    "    # Get element list\n",
    "    nodeList = data.getElementsByTagName(\"edge\")\n",
    "    print(\"Number of edges are: \", len(nodeList))\n",
    "    \n",
    "    # Define and initilzie explored data\n",
    "    all_rows = []\n",
    "    \n",
    "    for node in nodeList: \n",
    "        # Get features of each edge\n",
    "        edge_ID = node.getAttribute(\"id\")\n",
    "        node_start = node.getAttribute(\"from\")\n",
    "        node_end = node.getAttribute(\"to\")\n",
    "        priority = node.getAttribute(\"priority\")\n",
    "        # Get element list of edge\n",
    "        subNodeList = node.getElementsByTagName(\"lane\")\n",
    "        \n",
    "        # Initialize features\n",
    "        speed = 0\n",
    "        length = 0\n",
    "        for subNode in subNodeList:\n",
    "            # Get features of each lane\n",
    "            speed += float(subNode.getAttribute(\"speed\"))\n",
    "        \n",
    "            length += float(subNode.getAttribute(\"length\"))\n",
    "        speed_avg = speed / len(subNodeList)\n",
    "        length_avg = length / len(subNodeList)\n",
    "        lane_num = len(subNodeList)\n",
    "            \n",
    "        newRow = {\"edge_id\": edge_ID, \"node_start\": node_start, \"node_end\": node_end, \"lane_num\":lane_num, \"speed\": speed_avg, \"length\": length_avg, \"priority\": priority}\n",
    "        all_rows.append(newRow)\n",
    "              \n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    return df\n",
    " \n",
    "    \n",
    "# Call function to export network information data\n",
    "df = network_data_exploration(\"test.net.xml\")\n",
    "df.head()\n",
    "df.to_csv('Manhattan_network_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a3d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build a dictionary that maps SUMO road network information into int format and is readable by Simulation Algorithm:\n",
    "# 1). Convert node ID of string structure to int structure: node_to_int,\n",
    "# 2). Convert edge ID of string structure to int structure: edge_to_int,\n",
    "# 3). Convert edge ID of int structure to string structure: int_to_edge,\n",
    "# 4). Store node ID and edge ID of int structure in Manhattan_network_mapped.csv.\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv('Manhattan_network_raw.csv')\n",
    "\n",
    "# Convert unique string ids of \"node_start\" and \"node_end\" to unique integers.\n",
    "# Get a list of unique nodes\n",
    "unique_nodes = pd.concat([df['node_start'], df['node_end']]).unique()\n",
    "# Create a mapping of node string id to integer\n",
    "node_to_int = {node: idx for idx, node in enumerate(unique_nodes)}\n",
    "# Replace the string ids in the dataframe\n",
    "df['node_start'] = df['node_start'].map(node_to_int)\n",
    "df['node_end'] = df['node_end'].map(node_to_int)\n",
    "\n",
    "# Convert \"edge_id\" to unique integers.\n",
    "# Create a mapping of edge string id to integer\n",
    "edge_to_int = {edge: idx for idx, edge in enumerate(df['edge_id'].unique())}\n",
    "# Replace the string ids in the dataframe\n",
    "df['edge_id'] = df['edge_id'].map(edge_to_int)\n",
    "\n",
    "# Reverse edge_to_int\n",
    "int_to_edge = {v: k for k, v in edge_to_int.items()}\n",
    "\n",
    "# Save mapped dataframe\n",
    "df.to_csv('Manhattan_network_mapped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f0192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data path is: /Users/xuzizhuo/Desktop/Main Folder/My_works/routeSys_Github/Model_Training_on_Manual_Allocation_Data/SUMO_Simulation/test.net.xml\n"
     ]
    }
   ],
   "source": [
    "# 3. Caputre connection information\n",
    "def parse_sumo_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    edges = {}\n",
    "    connections = []\n",
    "\n",
    "    for edge in root.findall('.//edge'):\n",
    "        edge_id = edge.get('id')\n",
    "        edges[edge_id] = {\n",
    "            'from': edge.get('from'),\n",
    "            'to': edge.get('to')\n",
    "        }\n",
    "\n",
    "    for connection in root.findall('.//connection'):\n",
    "        from_edge = connection.get('from')\n",
    "        to_edge = connection.get('to')\n",
    "        direction = connection.get('dir')  # Turning direction\n",
    "        connections.append((from_edge, to_edge, direction))\n",
    "\n",
    "    return edges, connections\n",
    "\n",
    "path = os.path.abspath('./../../../Realistic_Event_Data_Generation_Procedure/') \n",
    "xml_file = os.path.join(path,'test.net.xml') \n",
    "print('Data path is:', xml_file) \n",
    "\n",
    "edges, connections = parse_sumo_xml(xml_file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c256806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to .csv file\n",
    "\n",
    "def save_connections_to_csv(connections, edge_to_int, filename):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['from_edge', 'to_edge', 'direction'])\n",
    "        \n",
    "        for from_edge, to_edge, direction in connections:\n",
    "            from_edge_int = edge_to_int[from_edge]\n",
    "            to_edge_int = edge_to_int[to_edge]\n",
    "            writer.writerow([from_edge_int, to_edge_int, direction])\n",
    "\n",
    "edges, connections = parse_sumo_xml(xml_file)\n",
    "save_connections_to_csv(connections, edge_to_int, 'connections_to_directions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f002ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build the mapped int edge id and its static features file\n",
    "\n",
    "# Generate the dictionary edge_id_to_features of int structure edge ID -> lane_num, speed_limit, average_length,\n",
    "# store the dictionary edge_id_to_features to a csv file for easy reading in C++.\n",
    "\n",
    "# Read preprocessed data\n",
    "df = pd.read_csv('Manhattan_network_mapped.csv')\n",
    "\n",
    "# Initialize an empty dictionary to store information mapping integer edge_id to its features\n",
    "edge_id_to_features = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Stores lane_num, speed, and length information for the current edge_id\n",
    "    edge_id_to_features[row['edge_id']] = {\n",
    "        \"lane_num\": row['lane_num'],\n",
    "        \"speed\": row['speed'],\n",
    "        \"length\": row['length'],\n",
    "        \"edge_str\": int_to_edge[row['edge_id']]\n",
    "    }\n",
    "\n",
    "# Save the dictionary as a CSV file\n",
    "with open('edge_id_to_features.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"edge_id\", \"lane_num\", \"speed\", \"length\", \"edge_str\"])\n",
    "    for edge_id, features in edge_id_to_features.items():\n",
    "        writer.writerow([edge_id, features['lane_num'], features['speed'], features['length'], features['edge_str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e39829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge to Nodes Mapping: (7480410399, 42437990)\n",
      "Nodes to Edge Mapping: -1004369132#0\n"
     ]
    }
   ],
   "source": [
    "# 5. Map edge to node pairs & node pairs to edges\n",
    "\n",
    "# Build the dictionaries nodes_to_edge_raw and edge_to_nodes_raw:\n",
    "# 1). edge_to_nodes_raw: Maps from edge IDs of string structure to node IDs of string structure\n",
    "# 2). nodes_to_edge_raw: Maps from node IDs of string structure to edge IDs of string structure\n",
    "\n",
    "# Read .csv file\n",
    "df = pd.read_csv('Manhattan_network_raw.csv')\n",
    "\n",
    "# Initialize two dictionaries\n",
    "edge_to_nodes_raw = {}  # edge_id 映射到 node_start 和 node_end\n",
    "nodes_to_edge_raw = {}  # node_start 和 node_end 映射到 edge_id\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    edge_id = row['edge_id']\n",
    "    node_start = row['node_start']\n",
    "    node_end = row['node_end']\n",
    "\n",
    "    # Create a mapping from edge_id to node_start and node_end\n",
    "    edge_to_nodes_raw[edge_id] = (node_start, node_end)\n",
    "\n",
    "    # Create a mapping from node_start and node_end to edge_id\n",
    "    nodes_to_edge_raw[(node_start, node_end)] = edge_id\n",
    "\n",
    "# 测试输出\n",
    "print(\"Edge to Nodes Mapping:\", edge_to_nodes_raw[\"-1004369132#0\"])\n",
    "print(\"Nodes to Edge Mapping:\", nodes_to_edge_raw[(7480410399, 42437990)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ce9c105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Map edge to node pairs & node pairs to edges\n",
    "\n",
    "# Build the dictionaries nodes_to_edge_mapped and edge_to_nodes_mapped:\n",
    "# 1). edge_to_nodes_mapped: Maps from edge IDs of int structure to node IDs of int structure\n",
    "# 2). nodes_to_edge_mapped: Maps from node IDs of int structure to edge IDs of int structure\n",
    "\n",
    "# Read .csv file\n",
    "df = pd.read_csv('Manhattan_network_mapped.csv')\n",
    "\n",
    "# Initialize two dictionaries\n",
    "edge_to_nodes_mapped = {}  # edge_id 映射到 node_start 和 node_end\n",
    "nodes_to_edge_mapped = {}  # node_start 和 node_end 映射到 edge_id\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    edge_id = row['edge_id']\n",
    "    node_start = row['node_start']\n",
    "    node_end = row['node_end']\n",
    "\n",
    "    # Create a mapping from edge_id to node_start and node_end\n",
    "    edge_to_nodes_mapped[edge_id] = (node_start, node_end)\n",
    "\n",
    "    # Create a mapping from node_start and node_end to edge_id\n",
    "    nodes_to_edge_mapped[(node_start, node_end)] = edge_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52bb75b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unqiue nodes:  20859\n",
      "Length of unique edges:  29497\n"
     ]
    }
   ],
   "source": [
    "# 7. Save the SUMO road network structure as a file readable by the Simulation Algorithm\n",
    "\n",
    "\n",
    "# Paths for the input and output files\n",
    "csv_file_path = 'Manhattan_network_mapped.csv'\n",
    "txt_file_path1 = 'Manhattan_network_BJ.txt'\n",
    "txt_file_path2 = 'Manhattan_network_min_Travel_Time.txt'\n",
    "\n",
    "# Loading the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Calculating unique nodes and edges\n",
    "unique_nodes = pd.concat([df['node_start'], df['node_end']]).unique()\n",
    "unique_edges = df['edge_id'].unique()\n",
    "\n",
    "# The two integer values to be added at the beginning of each .txt file\n",
    "int_val_1 = len(unique_nodes)\n",
    "print(\"Length of unqiue nodes: \", len(unique_nodes))\n",
    "int_val_2 = len(unique_edges)\n",
    "print(\"Length of unique edges: \", len(unique_edges))\n",
    "\n",
    "# Generating the first .txt file\n",
    "with open(txt_file_path1, 'w') as txt_file1:\n",
    "    # Writing the two integers to the TXT file\n",
    "    txt_file1.write(f\"{int(int_val_1)} {int(int_val_2)}\\n\")\n",
    "    \n",
    "    # Iterating over rows in the DataFrame to format and write each row according to specifications\n",
    "    for index, row in df.iterrows():\n",
    "        txt_file1.write(f\"{int(row['node_start'])} {int(row['node_end'])} {int(row['edge_id'])} {round(row['length'],2)}\\n\")\n",
    "\n",
    "# Generating the second .txt file\n",
    "with open(txt_file_path2, 'w') as txt_file2:\n",
    "    # Writing the two integers to the TXT file\n",
    "    txt_file2.write(f\"{int(int_val_1)} {int(int_val_2)}\\n\")\n",
    "    \n",
    "    # Iterating over rows in the DataFrame to format and write each row according to specifications\n",
    "    for index, row in df.iterrows():\n",
    "        travel_time = round(row['length'] / row['speed'],2)  # Calculating travel time as length_avg / speed_avg\n",
    "        txt_file2.write(f\"{int(row['node_start'])} {int(row['node_end'])} {travel_time}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ba79b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of routes are:  192495\n",
      "Number of route is: 192495\n"
     ]
    }
   ],
   "source": [
    "# 8. Convert route data to a format readable by SUMO\n",
    "\n",
    "# SUMO can generate trajectory data after the simulation is completed.\n",
    "# The following code extracts the corresponding information of the trajectory data from the trajectory data.\n",
    "# Capture trajectory data from \".xml\" file to \".csv\" file\n",
    "\n",
    "def trajectory_information_capture(filename):\n",
    "    # Get the XML file address\n",
    "    path = os.path.abspath('./../../../Realistic_Event_Data_Generation_Procedure/') \n",
    "    data_path = os.path.join(path,filename) \n",
    "    \n",
    "    # Open XML document\n",
    "    DOMTree = xml.dom.minidom.parse(data_path) \n",
    "    # According to the XML document, get the object of the document element\n",
    "    data = DOMTree.documentElement \n",
    "    \n",
    "    # Get the node list\n",
    "    nodeList = data.getElementsByTagName(\"vehicle\")\n",
    "    # Define the number of routes\n",
    "    nodeLen = len(nodeList)\n",
    "    print(\"Length of routes are: \", nodeLen)\n",
    "    \n",
    "    all_rows = []\n",
    "    routeID_set = set()\n",
    "    for node in nodeList: \n",
    "        # Get the current node attribute value\n",
    "        route_ID = node.getAttribute(\"id\")\n",
    "        depart_time = node.getAttribute(\"depart\")\n",
    "        subNodeList = node.getElementsByTagName(\"route\")\n",
    "        \n",
    "        for subNode in subNodeList:\n",
    "            if len(subNodeList) != 1:\n",
    "                print(\"Error.\")\n",
    "            \n",
    "            # Get the current node attribute value\n",
    "            route = subNode.getAttribute(\"edges\")\n",
    "            # Splitting a string\n",
    "            ids = route.split()\n",
    "            # Extract the first and last ID\n",
    "            start_node = ids[0]\n",
    "            end_node = ids[-1]\n",
    "        \n",
    "        newRow = {\"route_id\": route_ID, \"depart_time\": depart_time,\n",
    "                  \"start_edge\": start_node, \"end_edge\": end_node, \"route_by_edge\": route}\n",
    "        routeID_set.add(route_ID)\n",
    "        all_rows.append(newRow)\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    print(f\"Number of route is: {len(routeID_set)}\")\n",
    "    return df\n",
    "\n",
    "df_trajectory = trajectory_information_capture(\"reRoute.trips.xml\")\n",
    "df_trajectory.to_csv('Manhattan_trajectory_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a8f960b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>depart_time</th>\n",
       "      <th>start_edge</th>\n",
       "      <th>end_edge</th>\n",
       "      <th>route_by_edge</th>\n",
       "      <th>route_by_mappped_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>675967322#0</td>\n",
       "      <td>60813173#1</td>\n",
       "      <td>675967322#0 675967322#1 1032459844#0 103245984...</td>\n",
       "      <td>4868.0 18093.0 5489.0 5490.0 2599.0 2256.0 225...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156994977#0</td>\n",
       "      <td>988987066#4</td>\n",
       "      <td>156994977#0 156994977#1 963575583#0 963575583#...</td>\n",
       "      <td>7321.0 7333.0 20074.0 20075.0 20076.0 20079.0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3557</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1088320653#1</td>\n",
       "      <td>420358898#6</td>\n",
       "      <td>-1088320653#1 -1088320653#0 -438390724 -108832...</td>\n",
       "      <td>296.0 295.0 2127.0 294.0 293.0 292.0 291.0 290...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1030</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5672977#0</td>\n",
       "      <td>421853810</td>\n",
       "      <td>5672977#0 5672977#1 5672977#2 993379229#3 9933...</td>\n",
       "      <td>5230.0 16941.0 16942.0 16943.0 20687.0 20688.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3047</td>\n",
       "      <td>2.0</td>\n",
       "      <td>464683316#3</td>\n",
       "      <td>542096278#3</td>\n",
       "      <td>464683316#3 464683316#4 1046750793#0 104675079...</td>\n",
       "      <td>7474.0 13025.0 5723.0 5724.0 13021.0 13019.0 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   route_id  depart_time     start_edge     end_edge  \\\n",
       "0      2644          0.0    675967322#0   60813173#1   \n",
       "1      2126          1.0    156994977#0  988987066#4   \n",
       "2      3557          1.0  -1088320653#1  420358898#6   \n",
       "3      1030          2.0      5672977#0    421853810   \n",
       "4      3047          2.0    464683316#3  542096278#3   \n",
       "\n",
       "                                       route_by_edge  \\\n",
       "0  675967322#0 675967322#1 1032459844#0 103245984...   \n",
       "1  156994977#0 156994977#1 963575583#0 963575583#...   \n",
       "2  -1088320653#1 -1088320653#0 -438390724 -108832...   \n",
       "3  5672977#0 5672977#1 5672977#2 993379229#3 9933...   \n",
       "4  464683316#3 464683316#4 1046750793#0 104675079...   \n",
       "\n",
       "                               route_by_mappped_node  \n",
       "0  4868.0 18093.0 5489.0 5490.0 2599.0 2256.0 225...  \n",
       "1  7321.0 7333.0 20074.0 20075.0 20076.0 20079.0 ...  \n",
       "2  296.0 295.0 2127.0 294.0 293.0 292.0 291.0 290...  \n",
       "3  5230.0 16941.0 16942.0 16943.0 20687.0 20688.0...  \n",
       "4  7474.0 13025.0 5723.0 5724.0 13021.0 13019.0 1...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Convert raw edge IDs constructed route into mapped node IDs\n",
    "# E.g. raw_edge_1 raw_edge_2 ... -> mapped_node_1 mapped_node_2 mapped_node_3\n",
    "\n",
    "# Read .csv file\n",
    "df = pd.read_csv('Manhattan_trajectory_raw.csv')\n",
    "df['route_by_mappped_node'] = None\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Split the route_by_edge string and apply the mapping\n",
    "    node_pairs = [edge_to_nodes_mapped[edge_to_int[edge_id]] for edge_id in row['route_by_edge'].split(' ') if edge_id in edge_to_int]\n",
    "\n",
    "    # Process node sequences to ensure that nodes are not repeated\n",
    "    node_sequence = []\n",
    "    for pair in node_pairs:\n",
    "        # Make sure the node is converted to a string\n",
    "        node_start = str(pair[0])\n",
    "        node_end = str(pair[1])\n",
    "\n",
    "        if not node_sequence or node_sequence[-1] != node_start:\n",
    "            node_sequence.append(node_start)\n",
    "        node_sequence.append(node_end)\n",
    "\n",
    "    # Update information\n",
    "    df.at[index, 'route_by_mappped_node'] = ' '.join(node_sequence)\n",
    "\n",
    "# save to .csv file\n",
    "df.to_csv('Manhattan_trajectory_mapped_node.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "611c3944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5s/rs3sd7x95x9cf24kymvs99d40000gn/T/ipykernel_48738/1164579865.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Travel_Time'] = pd.to_numeric(df['Travel_Time'], errors='coerce') + pd.to_numeric(df['Delay_Time'], errors='coerce') + pd.to_numeric(df['LowSpee_Time'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# 10. Read relistic event data \n",
    "\n",
    "# Define data path\n",
    "path = os.path.abspath('./../../../Realistic_Event_Data_Generation_Procedure/') \n",
    "input_csv_path = os.path.join(path, 'TraCI_output_adjusted.csv')\n",
    "\n",
    "# Read the CSV data\n",
    "df_ETA = pd.read_csv(input_csv_path)\n",
    "df = df_ETA[['Vehicle_ID', 'E_Length', 'Edge_ID', 'Speed_Net', 'Time', 'Travel_Time', 'Delay_Time', 'LowSpee_Time']]\n",
    "\n",
    "# Add Wait_Sum to Travel_Time\n",
    "df['Travel_Time'] = pd.to_numeric(df['Travel_Time'], errors='coerce') + pd.to_numeric(df['Delay_Time'], errors='coerce') + pd.to_numeric(df['LowSpee_Time'], errors='coerce')\n",
    "# Travel time on tiny road segment should be zero.\n",
    "df.loc[df['E_Length'] / df['Speed_Net'] < 1, 'Travel_Time'] = 0\n",
    "\n",
    "# Find the average Travel_Time for different Edge_IDs and build a dictionary with the key being Edge_ID and the value being the average Travel_Time\n",
    "average_travel_time_dict = df.groupby('Edge_ID')['Travel_Time'].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983b5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36b64b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ece614cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192484\n"
     ]
    }
   ],
   "source": [
    "# 11. Verify: \n",
    "# Whether the route data contained in the trajectory file corresponds to the correctness of the route data,\n",
    "# converted by TraCI\n",
    "\n",
    "# Read trajectory .csv data\n",
    "df = pd.read_csv('Manhattan_trajectory_raw.csv')\n",
    "\n",
    "# Initialize dictionary\n",
    "route_dict = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    route_id = row['route_id']\n",
    "    edge_ids = row['route_by_edge'].split(' ')  # 将 edge ids 拆分成列表\n",
    "    route_dict[route_id] = edge_ids\n",
    "\n",
    "# 输出词典\n",
    "print(len(route_dict))\n",
    "\n",
    "# Define data path\n",
    "path = os.path.abspath('./../../../Realistic_Event_Data_Generation_Procedure/') \n",
    "data_path = os.path.join(path, 'TraCI_output_adjusted.csv')\n",
    "# Read realistic event data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Compute 'Travel_Time' as the sum of the three columns\n",
    "df['Travel_Time'] = pd.to_numeric(df['Travel_Time'], errors='coerce') + pd.to_numeric(df['Delay_Time'], errors='coerce') + pd.to_numeric(df['LowSpee_Time'], errors='coerce')\n",
    "\n",
    "# Set the Travel_Time to 0 for rows where E_Length / Speed_Net is less than 1\n",
    "df.loc[df['E_Length'] / df['Speed_Net'] < 1, 'Travel_Time'] = 0\n",
    "\n",
    "# Convert 'Delay_Time', 'LowSpee_Time', 'Wait_Time' to 0 and 1\n",
    "df[['Delay_Time', 'LowSpee_Time', 'Wait_Time']] = df[['Delay_Time', 'LowSpee_Time', 'Wait_Time']].applymap(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "row_count = 0\n",
    "    \n",
    "# Initialize dictionary\n",
    "vehicle_edge_time_dict = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    vehicle_id = row['Vehicle_ID']\n",
    "    edge_id = row['Edge_ID']\n",
    "    \n",
    "    # Building the inner dictionary\n",
    "    time_data = {\n",
    "        'Delay_Time': row['Delay_Time'],\n",
    "        'LowSpee_Time': row['LowSpee_Time'],\n",
    "        'Wait_Time': row['Wait_Time'],\n",
    "        'Travel_Time': row['Travel_Time'],\n",
    "        'Driving_Num': row['Driving_Num']\n",
    "    }\n",
    "    \n",
    "    # If the vehicle_id already exists in the dictionary\n",
    "    if vehicle_id in vehicle_edge_time_dict:\n",
    "        # If edge_id has not been recorded yet, add it\n",
    "        if edge_id not in vehicle_edge_time_dict[vehicle_id]:\n",
    "            vehicle_edge_time_dict[vehicle_id][edge_id] = time_data\n",
    "        else:\n",
    "            # If edge_id already exists, you can choose to update the data or skip\n",
    "            print(f\"Edge {edge_id} 已存在于 vehicle {vehicle_id} 中\")\n",
    "    else:\n",
    "        # If the vehicle_id does not exist, initialize a new dictionary\n",
    "        vehicle_edge_time_dict[vehicle_id] = {edge_id: time_data}\n",
    "\n",
    "    row_count += 1\n",
    "\n",
    "    # Print process procedure\n",
    "    if row_count % 5000000 == 0:\n",
    "        print(f\"已处理 {row_count} 行数据\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c148794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证1:词典长度一致\n",
      "验证2.1 : 检查 route_dict 中的 edge ids 是否包含重复:\n",
      "验证3: 检查 route_dict 是否包含 vehicle_edge_time_dict 的 edge ids:\n"
     ]
    }
   ],
   "source": [
    "# 12. Correctness check\n",
    "\n",
    "# 1). Verify that two dictionaries have the same length\n",
    "if len(route_dict) != len(vehicle_edge_time_dict):\n",
    "    print(f\"Error: two dic length are different - route_dict length: {len(route_dict)}, vehicle_edge_time_dict length: {len(vehicle_edge_time_dict)}\")\n",
    "\n",
    "# 2). Verify that the edge ids for each route_id contain duplicates\n",
    "def check_duplicates(route_dict):\n",
    "    for route_id, edge_ids in route_dict.items():\n",
    "        if len(edge_ids) != len(set(edge_ids)):\n",
    "            print(f\"Error: route_id {route_id} exists duplicated edge ids\")\n",
    "\n",
    "# 3). Check route_dict for duplicate edge ids\n",
    "print(\"Check if the edge ids in route_dict contain duplicates:\")\n",
    "check_duplicates(route_dict)\n",
    "\n",
    "# 4). Verify that the edge ids in route_dict contain the edge ids in vehicle_edge_time_dict\n",
    "print(\"Check if route_dict contains the edge ids from vehicle_edge_time_dict:\")\n",
    "for route_id, edge_dict in vehicle_edge_time_dict.items():\n",
    "    if route_id in route_dict:\n",
    "        route_edges = set(route_dict[route_id])\n",
    "        vehicle_edges = set(edge_dict.keys())\n",
    "        \n",
    "        # 如果 route_dict 的 edge ids 不完全包含 vehicle_edge_time_dict 的 edge ids，打印错误\n",
    "        if not vehicle_edges.issubset(route_edges):\n",
    "            print(f\"Error: route_id {route_id}'s edge ids do not all contained in route_dict .\")\n",
    "    else:\n",
    "        print(f\"Error: route_id {route_id} does not exit in route_dict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30f5596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Complete data\n",
    "\n",
    "\n",
    "# Traverse route_id and process edge ids under each route_id\n",
    "for route_id in route_dict:\n",
    "    \n",
    "    # Get edge ids in route_dict\n",
    "    route_edges = route_dict[route_id]  # 这个是完整的 edge ids 列表\n",
    "    \n",
    "    # Check if the route_id exists in vehicle_edge_time_dict\n",
    "    if route_id in vehicle_edge_time_dict:\n",
    "        # Get the edge ids under the current route_id in vehicle_edge_time_dict\n",
    "        vehicle_edges = list(vehicle_edge_time_dict[route_id].keys())\n",
    "    else:\n",
    "        # If the route_id does not exist in vehicle_edge_time_dict, initialize an empty nested dictionary\n",
    "        print(f\"Error: route_id {route_id} 不存在于 vehicle_edge_time_dict 中\")\n",
    "        vehicle_edge_time_dict[route_id] = {}\n",
    "        vehicle_edges = []\n",
    "\n",
    "    # Find missing edge ids: edge ids that are in route_dict but not in vehicle_edge_time_dict\n",
    "    missing_edges = [edge_id for edge_id in route_edges if edge_id not in vehicle_edges]\n",
    "    \n",
    "    # For missing edge ids, complete the values ​​in vehicle_edge_time_dict\n",
    "    for edge_id in missing_edges:\n",
    "        # Initialized to 0 value, including all fields\n",
    "        vehicle_edge_time_dict[route_id][edge_id] = {\n",
    "            'Delay_Time': 0,\n",
    "            'LowSpee_Time': 0,\n",
    "            'Wait_Time': 0,\n",
    "            'Travel_Time': 0,\n",
    "            'Driving_Num': 1,\n",
    "        }\n",
    "\n",
    "    # 5. Keep order: Arrange the edge ids of vehicle_edge_time_dict in the order of route_dict\n",
    "    # We first get the complete list of edge ids in order\n",
    "    ordered_edges = {edge_id: vehicle_edge_time_dict[route_id][edge_id] for edge_id in route_edges}\n",
    "    # Reassign the ordered edge ids back to vehicle_edge_time_dict\n",
    "    vehicle_edge_time_dict[route_id] = ordered_edges\n",
    "\n",
    "# Finally, check the result to confirm whether the edge ids of each route_id have been completed.\n",
    "for route_id in vehicle_edge_time_dict:\n",
    "    if not set(vehicle_edge_time_dict[route_id].keys()) == set(route_dict[route_id]):\n",
    "        print(f\"Error: route_id {route_id} 未能成功补全\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177c409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f75680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Generates both route data and time data\n",
    "\n",
    "# In route.txt, the first value of each line is the node length, \n",
    "# followed by the start_node and the corresponding time bool information\n",
    "\n",
    "# In time.txt, the first value of each line is the vehicle id, the length of the travel time value, \n",
    "# and the following travel time value as truth\n",
    "\n",
    "# Sort vehicle_edge_time_dict by route_id\n",
    "vehicle_edge_time_dict = dict(sorted(vehicle_edge_time_dict.items(), key=lambda item: item[0]))\n",
    "\n",
    "# Open the route.txt and time.txt file for writing\n",
    "with open('route.txt', 'w') as route_file, open('time.txt', 'w') as time_file:\n",
    "    \n",
    "    # Iterate over each route_id in vehicle_edge_time_dict\n",
    "    for idx, (route_id, edge_time_dict) in enumerate(vehicle_edge_time_dict.items()):\n",
    "        \n",
    "        # Determine whether idx is equal to route_id\n",
    "        if idx+1 != route_id:\n",
    "            print(f\"Mismatch: idx {idx} does not match route_id {route_id}\")\n",
    "        \n",
    "        # Get a list of all edge ids under the route_id\n",
    "        edge_ids = list(edge_time_dict.keys())\n",
    "        \n",
    "        # The length of edge ids written to the route.txt file\n",
    "        route_file.write(f\"{len(edge_ids) + 1} \")  # +1 because we need to write to second_node last\n",
    "        \n",
    "        # The length of the route_id and edge ids written to the time.txt file\n",
    "        time_file.write(f\"{len(edge_ids)} \")\n",
    "        \n",
    "        # Traverse each edge_id and get the corresponding time data\n",
    "        for edge_id in edge_ids:\n",
    "            # Use edge_to_int to convert edge_id to an integer\n",
    "            if edge_id in edge_to_int:\n",
    "                edge_int = edge_to_int[edge_id]\n",
    "            else:\n",
    "                # If edge_id is not in edge_to_int, print an error message and skip the edge_id\n",
    "                print(f\"Error: edge_id {edge_id} 不存在于 edge_to_int 词典中\")\n",
    "                continue  # Skip this edge_id\n",
    "            \n",
    "            # Use edge_to_nodes_mapped dictionary to convert edge_int to node pair\n",
    "            if edge_int in edge_to_nodes_mapped:\n",
    "                first_node, second_node = edge_to_nodes_mapped[edge_int]\n",
    "            else:\n",
    "                # If edge_int is not in edge_to_nodes_mapped, print an error message and skip the edge_id\n",
    "                print(f\"Error: edge_int {edge_int} 不存在于 edge_to_nodes_mapped 词典中\")\n",
    "                continue  # Skip this edge_id\n",
    "            \n",
    "            # Get the time data of the edge\n",
    "            delay_time = edge_time_dict[edge_id]['Delay_Time']\n",
    "            low_speed_time = edge_time_dict[edge_id]['LowSpee_Time']\n",
    "            wait_time = edge_time_dict[edge_id]['Wait_Time']\n",
    "            travel_time = edge_time_dict[edge_id]['Travel_Time']\n",
    "            driving_num = edge_time_dict[edge_id]['Driving_Num']\n",
    "            \n",
    "            # Write first_node and its Delay_Time, LowSpee_Time, Wait_Time to the route.txt file\n",
    "            route_file.write(f\"{int(first_node)} {delay_time} {low_speed_time} {wait_time} {driving_num} \")\n",
    "            \n",
    "            # Write Travel_Time to time.txt file\n",
    "            time_file.write(f\"{travel_time} \")\n",
    "        \n",
    "        # The second_node of the last edge is written into the route.txt file. \n",
    "        # All three time values are assigned 0, and the last value is 1 as the minimum driving number.\n",
    "        if edge_ids:\n",
    "            route_file.write(f\"{int(second_node)} 0 0 0 1\")\n",
    "        \n",
    "        # Wrap after each route is processed\n",
    "        route_file.write('\\n')\n",
    "        time_file.write('\\n')\n",
    "\n",
    "print(\"Data has been written to route.txt and time.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80977cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to route_edge.txt\n"
     ]
    }
   ],
   "source": [
    "# 15. Generate route_edge.txt file\n",
    "\n",
    "# Open the route.txt file for writing\n",
    "with open('route_edge.txt', 'w') as route_file:\n",
    "    \n",
    "    # Iterate over each route_id in vehicle_edge_time_dict\n",
    "    for idx, (route_id, edge_time_dict) in enumerate(vehicle_edge_time_dict.items()):\n",
    "        \n",
    "        # Determine whether idx is equal to route_id\n",
    "        if idx+1 != route_id:\n",
    "            print(f\"Mismatch: idx {idx} does not match route_id {route_id}\")\n",
    "        \n",
    "        # Get a list of all edge ids under the route_id\n",
    "        edge_ids = list(edge_time_dict.keys())\n",
    "        \n",
    "        # The length of edge ids written to the route.txt file\n",
    "        route_file.write(f\"{len(edge_ids)} \")\n",
    "        \n",
    "        # Traverse each edge_id and get the corresponding time data\n",
    "        for edge_id in edge_ids:\n",
    "            \n",
    "            # Write first_node and its Delay_Time, LowSpee_Time, Wait_Time to the route.txt file\n",
    "            route_file.write(f\"{edge_id} \")\n",
    "        \n",
    "        # Wrap after each route is processed\n",
    "        route_file.write('\\n')\n",
    "\n",
    "print(\"Data has been written to route_edge.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a69e2495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to query.txt\n"
     ]
    }
   ],
   "source": [
    "# 16. Generate query.txt file\n",
    "\n",
    "# Read csv file\n",
    "df = pd.read_csv('Manhattan_trajectory_mapped_node.csv')\n",
    "df = df.sort_values(by='route_id')\n",
    "\n",
    "# Open a new text file for writing data\n",
    "with open('query.txt', 'w') as file:\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract the origin and destination nodes and convert them to integers\n",
    "        node_sequence = row['route_by_mappped_node'].split()\n",
    "        departure_node = int(float(node_sequence[0]))  # Convert to floating point first, then to integer\n",
    "        destination_node = int(float(node_sequence[-1]))\n",
    "\n",
    "        # Extract the departure time and convert it to an integer\n",
    "        departure_time = int(row['depart_time'])\n",
    "\n",
    "        # Write the extracted information into a file in the format\n",
    "        file.write(f\"{departure_node} {destination_node} {departure_time}\\n\")\n",
    "\n",
    "print(\"Data has been written to query.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
